{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1V0PEIoUhq40rLRl7YaD7wIpwfQQcNd-T",
      "authorship_tag": "ABX9TyND7G/VX7NLYcWVLzHYXD/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raljun/data-analysis/blob/main/GroupProject18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Group 18\n",
        "\n",
        "```\n",
        " Gestures Recognition\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ernakRy6NOvR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0I6TeQiKbcr"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o data.zip\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bhwvzaC4LBUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Lister le dossier pour vérifier que le fichier existe\n",
        "if os.path.exists(\"data/Domain1_csv/Subject1-0-1.csv\"):\n",
        "    print(\"✔️ Dataset found\")\n",
        "else:\n",
        "    print(\"❌ Fichier manquant : data/Domain1_csv/Subject1-0-1.csv\")"
      ],
      "metadata": {
        "id": "4Ppx2GpPLNDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Import of Libraries\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sk5hPNNXOExr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.interpolate as interp\n",
        "import numpy as np\n",
        "from numba import njit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "PpF6P8kTOwd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  # nécessaire pour vérifier l'existence des fichiers\n",
        "\n",
        "def read_files(domain):\n",
        "    \"\"\"\n",
        "    Reads gesture CSV files from a given domain and returns:\n",
        "    - data: list of 3D point sequences (x, y, z)\n",
        "    - points_array: list of full sequences (x, y, z, t)\n",
        "    - seq_array: corresponding digit labels\n",
        "    \"\"\"\n",
        "    data = []         # stores 3D coordinates only\n",
        "    svm_points = []   # stores all columns (x, y, z, t)\n",
        "    y_svm = []        # stores digit labels\n",
        "\n",
        "    for a in range(1, 11):      # user ID 1–10\n",
        "        for b in range(0, 10):  # digit class 0–9\n",
        "            for c in range(1, 11):  # repetition 1–10\n",
        "                direc = f\"data/Domain{domain}_csv/Subject{a}-{b}-{c}.csv\"\n",
        "\n",
        "                # Vérification optionnelle\n",
        "                if not os.path.exists(direc):\n",
        "                  print(f\"File not found: {direc}\")\n",
        "                  continue\n",
        "\n",
        "\n",
        "                df = pd.read_csv(direc)\n",
        "                y_svm.append(b)\n",
        "                svm_points.append(np.array(df.values))              # full data: x, y, z, t\n",
        "                data.append(np.array(df.iloc[:, 0:3].values))       # only x, y, z\n",
        "\n",
        "    points_array = np.array(svm_points, dtype=object)\n",
        "    seq_array = np.array(y_svm, dtype=object)\n",
        "\n",
        "    return data, points_array, seq_array\n",
        "\n",
        "@njit()\n",
        "def euclidean_distance(x, y):\n",
        "    \"\"\"Compute Euclidean distance between two vectors x and y\"\"\"\n",
        "    return np.sqrt(np.sum((x - y)**2))\n",
        "\n",
        "@njit(nogil=True)\n",
        "def dtw(serie1, serie2):\n",
        "    \"\"\"\n",
        "    Compute the DTW distance between two sequences of 3D points.\n",
        "    @pre: serie1 and serie2 are numpy arrays of shape (n, 3)\n",
        "    @post: returns the DTW alignment cost\n",
        "    \"\"\"\n",
        "    l1, l2 = len(serie1), len(serie2)\n",
        "    cost_matrix = np.full((l1 + 1, l2 + 1), np.inf)\n",
        "    cost_matrix[0, 0] = 0.0\n",
        "\n",
        "    for i in range(l1):\n",
        "        for j in range(l2):\n",
        "            cost = euclidean_distance(serie1[i], serie2[j])\n",
        "            cost_matrix[i+1, j+1] = cost + min(\n",
        "                cost_matrix[i, j+1],\n",
        "                cost_matrix[i+1, j],\n",
        "                cost_matrix[i, j]\n",
        "            )\n",
        "\n",
        "    return cost_matrix[-1, -1]\n",
        "\n",
        "def plot_conf_mat(labels, pred_labels, user_id):\n",
        "    \"\"\"\n",
        "    Plot and save the confusion matrix for a given user's predictions.\n",
        "    @pre: labels – true labels (list or array)\n",
        "          pred_labels – predicted labels\n",
        "          user_id – integer, ID of the user\n",
        "    @post: Saves a PNG file of the confusion matrix.\n",
        "    \"\"\"\n",
        "    conf_mat = confusion_matrix(labels, pred_labels)\n",
        "    df_cm = pd.DataFrame(conf_mat, range(10), range(10))\n",
        "    sn.set(font_scale=1.2)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sn.heatmap(df_cm, annot=True)       ## cmap=\"Blues\"\n",
        "    plt.title(f\"User {user_id+1} – Confusion Matrix\")\n",
        "    plt.savefig(f\"confusion_matrix_user{user_id+1}.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def cross_validation_user_independent_split(X, y, i):\n",
        "    X = np.array(X, dtype=object)\n",
        "    y = np.array(y, dtype=object)\n",
        "\n",
        "    test_idx = slice((i-1)*100, i*100)\n",
        "    test_points = X[test_idx]\n",
        "    test_labels = y[test_idx]\n",
        "\n",
        "    train_mask = np.ones(len(X), dtype=bool)\n",
        "    train_mask[test_idx] = False\n",
        "    train_points = X[train_mask]\n",
        "    train_labels = y[train_mask]\n",
        "\n",
        "    return train_points, test_points, train_labels, test_labels\n",
        "\n",
        "def cross_validation_user_dependent_split(X, y, n):\n",
        "    \"\"\"\n",
        "    Performs a user-dependent cross-validation split:\n",
        "    for each digit (0–9), selects the nth repetition (n=1 to 10) as test,\n",
        "    and the 9 others as training.\n",
        "\n",
        "    @pre: X and y contain 100 samples for one user (10 digits × 10 repetitions)\n",
        "    @param n: repetition index to use for test samples (1 ≤ n ≤ 10)\n",
        "    @return: training points, test points, training labels, test labels\n",
        "    \"\"\"\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Indices of the nth repetition for each digit\n",
        "    test_idx = [(n - 1) + 10 * i for i in range(10)]\n",
        "\n",
        "    # Split data\n",
        "    test_points = X[test_idx]\n",
        "    test_labels = y[test_idx]\n",
        "\n",
        "    train_mask = np.ones(100, dtype=bool)\n",
        "    train_mask[test_idx] = False\n",
        "    train_points = X[train_mask]\n",
        "    train_labels = y[train_mask]\n",
        "\n",
        "    return train_points, test_points, train_labels, test_labels\n",
        "\n",
        "def get_user(X, y, user, limit=100):\n",
        "    \"\"\"\n",
        "    Extract the gesture data and labels for a specific user.\n",
        "    @param user: user ID (1 to 10)\n",
        "    @param limit: number of samples per user (default: 100)\n",
        "    @return: points_array (sequences), seq_array (labels)\n",
        "    \"\"\"\n",
        "    X = np.array(X, dtype=object)\n",
        "    y = np.array(y, dtype=object)\n",
        "    indexes = range((user - 1) * limit, user * limit)\n",
        "    return X[indexes], y[indexes]\n",
        "\n",
        "def resampling(sequence, n_new=80):\n",
        "    \"\"\"\n",
        "    Resample a time series (3D + time) to a fixed number of points using linear interpolation.\n",
        "    @param sequence: numpy array of shape (n_old, 4) with columns [x, y, z, t]\n",
        "    @param n_new: number of points in the resampled sequence\n",
        "    @return: array of shape (n_new, 4)\n",
        "    \"\"\"\n",
        "    n_old, m = sequence.shape\n",
        "    mat_new = np.zeros((n_new, m))\n",
        "\n",
        "    x_old = np.asarray(sequence[:, 3]).squeeze()\n",
        "    x_new = np.linspace(sequence[:, 3].min(), sequence[:, 3].max(), n_new)\n",
        "\n",
        "    for j in range(m - 1):  # interpolate x, y, z\n",
        "        y_old = np.asarray(sequence[:, j]).squeeze()\n",
        "        interpolator = interp.interp1d(x_old, y_old, fill_value=\"extrapolate\")\n",
        "        mat_new[:, j] = interpolator(x_new)\n",
        "\n",
        "    mat_new[:, -1] = x_new  # put time in last column\n",
        "    return mat_new\n",
        "\n",
        "def get_acc(X):\n",
        "    \"\"\"\n",
        "    Compute the instantaneous acceleration magnitude from a 3D time series.\n",
        "\n",
        "    Parameters:\n",
        "        X (ndarray): Array of shape (n, 4) with columns [x, y, z, t],\n",
        "                     where t is time.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A 1D array of acceleration magnitudes at each time step.\n",
        "    \"\"\"\n",
        "    dt = np.gradient(X[:, 3])\n",
        "    dx_dt = np.gradient(X[:, 0]) / dt\n",
        "    dy_dt = np.gradient(X[:, 1]) / dt\n",
        "    dz_dt = np.gradient(X[:, 2]) / dt\n",
        "\n",
        "    d2x_dt2 = np.gradient(dx_dt) / dt\n",
        "    d2y_dt2 = np.gradient(dy_dt) / dt\n",
        "    d2z_dt2 = np.gradient(dz_dt) / dt\n",
        "\n",
        "    acceleration = np.sqrt(d2x_dt2**2 + d2y_dt2**2 + d2z_dt2**2)\n",
        "    return acceleration\n",
        "\n",
        "\n",
        "def get_angle(X):\n",
        "    \"\"\"\n",
        "    Compute the angular orientation of the gesture's acceleration vector\n",
        "    in the YZ plane with respect to the X axis.\n",
        "\n",
        "    Parameters:\n",
        "        X (ndarray): Array of shape (n, 4), with [x, y, z, t].\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A 1D array of angles (in radians) at each time step.\n",
        "    \"\"\"\n",
        "    dt = np.gradient(X[:, 3])\n",
        "    dx_dt = np.gradient(X[:, 0]) / dt\n",
        "    dy_dt = np.gradient(X[:, 1]) / dt\n",
        "    dz_dt = np.gradient(X[:, 2]) / dt\n",
        "\n",
        "    d2x_dt2 = np.gradient(dx_dt) / dt\n",
        "    d2y_dt2 = np.gradient(dy_dt) / dt\n",
        "    d2z_dt2 = np.gradient(dz_dt) / dt\n",
        "\n",
        "    angle = np.arctan2(np.sqrt(d2y_dt2**2 + d2z_dt2**2), np.abs(d2x_dt2))\n",
        "    return angle\n",
        "\n",
        "def get_kinetic_energy(X, mass):\n",
        "    \"\"\"\n",
        "    Compute the kinetic energy at each time step of a gesture trajectory.\n",
        "\n",
        "    Parameters:\n",
        "        X (ndarray): Array of shape (n, 4), with [x, y, z, t].\n",
        "        mass (float): Constant mass of the moving object.\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A 1D array of kinetic energy values at each time step.\n",
        "    \"\"\"\n",
        "    dt = np.gradient(X[:, 3])\n",
        "    dx_dt = np.gradient(X[:, 0]) / dt\n",
        "    dy_dt = np.gradient(X[:, 1]) / dt\n",
        "    dz_dt = np.gradient(X[:, 2]) / dt\n",
        "\n",
        "    velocity = np.sqrt(dx_dt**2 + dy_dt**2 + dz_dt**2)\n",
        "    kinetic_energy = 0.5 * mass * velocity**2\n",
        "    return kinetic_energy\n",
        "\n",
        "\n",
        "def get_speed(X):\n",
        "    \"\"\"\n",
        "    Compute the instantaneous speed of a 3D gesture trajectory.\n",
        "\n",
        "    Parameters:\n",
        "        X (ndarray): Array of shape (n, 4), where the columns are [x, y, z, t].\n",
        "\n",
        "    Returns:\n",
        "        ndarray: A 1D array of speed magnitudes at each time step.\n",
        "    \"\"\"\n",
        "    dt = np.gradient(X[:, 3])\n",
        "    dx_dt = np.gradient(X[:, 0]) / dt\n",
        "    dy_dt = np.gradient(X[:, 1]) / dt\n",
        "    dz_dt = np.gradient(X[:, 2]) / dt\n",
        "\n",
        "    speed = np.sqrt(dx_dt**2 + dy_dt**2 + dz_dt**2)\n",
        "    return speed\n",
        "\n"
      ],
      "metadata": {
        "id": "ab3sbX7NxMf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Dynamic time warping(DWT) and K-Nearest Neighbors(KNN)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "F5GLvTu8Pfuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numba.typed import List\n",
        "from sklearn.metrics import accuracy_score\n",
        "from joblib import Parallel, delayed"
      ],
      "metadata": {
        "id": "859bkRn-MlvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN_DTW:\n",
        "    \"\"\"\n",
        "    K-Nearest Neighbors classifier using DTW distance for time series data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_neighbors=1):\n",
        "        self.n_neighbors = n_neighbors\n",
        "\n",
        "    def fit(self, x, labels):\n",
        "        \"\"\"\n",
        "        Store training sequences and their labels.\n",
        "\n",
        "        Parameters:\n",
        "            x (array-like): list or array of time series\n",
        "            labels (array-like): list of class labels\n",
        "        \"\"\"\n",
        "        self.x_train = np.array(x)\n",
        "        self.labels = np.array(labels)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "        \"\"\"\n",
        "        Predict labels for the test set using DTW-based KNN.\n",
        "\n",
        "        Parameters:\n",
        "            x_test (array-like): list of time series to classify\n",
        "\n",
        "        Returns:\n",
        "            ndarray: predicted labels\n",
        "        \"\"\"\n",
        "        matrix = Parallel(n_jobs=-1, prefer=\"threads\", verbose=0)(\n",
        "            delayed(dtw)(\n",
        "                np.array(x_test[i], dtype=np.float64),\n",
        "                np.array(self.x_train[j], dtype=np.float64)\n",
        "            )\n",
        "            for i in range(len(x_test)) for j in range(len(self.x_train))\n",
        "        )\n",
        "\n",
        "        dist_matrix = np.array(matrix).reshape((len(x_test), -1))\n",
        "        indexes = dist_matrix.argsort()[:, :self.n_neighbors]\n",
        "        neighbor_labels = self.labels[indexes]\n",
        "        return pd.DataFrame(neighbor_labels).mode(axis=1).iloc[:, 0].to_numpy()\n",
        "\n",
        "def test(train_set, test_set, train_labels, test_labels, model):\n",
        "  \"\"\"\n",
        "  Train a KNN model and evaluate it on test data.\n",
        "\n",
        "  Parameters:\n",
        "  train_set (array-like): sequences used for training\n",
        "  test_set (array-like): sequences used for testing\n",
        "  train_labels (array-like): true labels for training data\n",
        "  test_labels (array-like): true labels for test data\n",
        "  model: classifier implementing fit() and predict()\n",
        "\n",
        "  Returns:\n",
        "    Tuple:\n",
        "      - float: accuracy score\n",
        "      - ndarray: predicted labels\n",
        "  \"\"\"\n",
        "  model.fit(train_set, train_labels)\n",
        "  predictions = model.predict(test_set)\n",
        "  y_true = np.array(test_labels, dtype=int).flatten()\n",
        "  y_pred = np.array(predictions, dtype=int).flatten()\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  return accuracy, predictions\n",
        "\n",
        "\n",
        "\n",
        "def validation_inter(dataset, labels, model, typeUser=\"independent\", user=0):\n",
        "  \"\"\"\n",
        "  Perform cross-validation either in user-independent or user-dependent mode.\n",
        "\n",
        "  Parameters:\n",
        "      dataset (array-like): list of gesture sequences\n",
        "      labels (array-like): list of gesture labels\n",
        "      model: a classifier with .fit() and .predict() methods\n",
        "      typeUser (str): \"independent\" or \"dependent\"\n",
        "      user (int): user ID (used only in dependent mode)\n",
        "\n",
        "  Side effects:\n",
        "      Appends accuracy and prediction results to global lists:\n",
        "      accuracies, predictions, labe\n",
        "  \"\"\"\n",
        "  for j in range(10):\n",
        "    if typeUser == \"independent\":\n",
        "      X_train, X_test, seq_train, seq_test = cross_validation_user_independent_split(dataset, labels, j + 1)\n",
        "    else:\n",
        "      X_train, X_test, seq_train, seq_test = cross_validation_user_dependent_split(dataset, labels, j + 1)\n",
        "\n",
        "    accuracy, prediction = test(X_train, X_test, seq_train, seq_test, model)\n",
        "    accuracies.append(accuracy)\n",
        "    predictions.append(prediction)\n",
        "    labe.append(np.array(seq_test, dtype=int))\n",
        "\n",
        "    if typeUser == \"independent\":\n",
        "      print(f\"The user independent score {j + 1}: {accuracy * 100:.1f}%\")\n",
        "    else:\n",
        "      print(f\"The user dependent score {user} using the {j + 1} try of each gesture: {accuracy * 100:.1f}%\")\n",
        "\n",
        "def validationInd(dataset, labels, model):\n",
        "  \"\"\"\n",
        "  Run user-independent validation and return the results.\n",
        "\n",
        "  Parameters:\n",
        "    dataset (array-like): list of gesture sequences\n",
        "    labels (array-like): corresponding gesture labels\n",
        "    model: a KNN_DTW model or compatible classifier\n",
        "\n",
        "    Returns:\n",
        "     tuple: (accuracies, predictions, true_labels) for all users\n",
        "  \"\"\"\n",
        "  dataset = np.array(dataset)\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  print(\"\\n\\t******\\tUSER INDEPENDENT\\t******\\t\\n\")\n",
        "  validation_inter(dataset, labels, model, typeUser=\"independent\")\n",
        "  return accuracies, predictions, labe\n",
        "\n",
        "def validationDep(dataset, labels, model):\n",
        "  \"\"\"\n",
        "  Run user-dependent validation and return the results.\n",
        "\n",
        "  Parameters:\n",
        "    dataset (array-like): full list of gesture sequences\n",
        "    labels (array-like): full list of labels\n",
        "    model: classifier with fit and predict methods\n",
        "\n",
        "    Returns:\n",
        "      tuple: (accuracies, predictions, true_labels) for all users\n",
        "  \"\"\"\n",
        "  dataset = np.array(dataset)\n",
        "  labels = np.array(labels)\n",
        "  print(\"\\n\\t******\\tUSER DEPENDENT\\t******\\t\\n\")\n",
        "\n",
        "  for i in range(10):  # for the 10 users\n",
        "    user_data, user_labels = get_user(dataset, labels, i + 1)\n",
        "    validation_inter(user_data, user_labels, model, typeUser=\"dependent\", user=i + 1)\n",
        "    return accuracies, predictions, labe\n",
        "\n",
        "def clear_globals():\n",
        "  \"\"\"\n",
        "  Reset global tracking lists for new validation runs.\n",
        "  \"\"\"\n",
        "  global accuracies, predictions, labe\n",
        "  from numba.typed import List\n",
        "  accuracies = List()\n",
        "  predictions = List()\n",
        "  labe = []\n",
        "\n",
        "def run_validation_pipeline(data, model):\n",
        "  \"\"\"\n",
        "  Runs both user-independent and user-dependent validations and prints results.\n",
        "  \"\"\"\n",
        "  clear_globals()\n",
        "\n",
        "  print(\"\\n=== USER-INDEPENDENT VALIDATION ===\")\n",
        "  acc_in, pred_in, labels_in = validationInd(np.array(data[0], dtype=object).T, np.array(data[2]), model)\n",
        "  print(f\"Average accuracy: {np.mean(acc_in)*100:.2f}%\")\n",
        "  print(f\"Standard deviation: {np.std(acc_in):.2f}\")\n",
        "  for user_id in range(len(pred_in)):\n",
        "    plot_conf_mat(labels_in[user_id], pred_in[user_id], user_id)\n",
        "\n",
        "\n",
        "  clear_globals()\n",
        "\n",
        "  print(\"\\n=== USER-DEPENDENT VALIDATION ===\")\n",
        "  acc_dep, pred_dep, labels_dep = validationDep(np.array(data[0], dtype=object).T, np.array(data[2]), model)\n",
        "  print(f\"Average accuracy: {np.mean(acc_dep)*100:.2f}%\")\n",
        "  print(f\"Standard deviation: {np.std(acc_dep):.2f}\")\n",
        "  for user_id in range(len(pred_dep)):\n",
        "    plot_conf_mat(labels_dep[user_id], pred_dep[user_id], user_id)\n",
        "\n"
      ],
      "metadata": {
        "id": "_lKkBc7W_Rwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "Simulation of DWT and KNN\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IZJ-tVMjO0BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model = KNN_DTW(n_neighbors=3)\n",
        "    data = read_files(1)\n",
        "    run_validation_pipeline(data, model)"
      ],
      "metadata": {
        "id": "I-3vVcsIM20V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Advanced method: Support Vector Machine(SVM)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "0m-2xflKO9fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC"
      ],
      "metadata": {
        "id": "o85nQi_FM4J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data):\n",
        "    \"\"\"\n",
        "    Extracts speed, acceleration, and angular features from resampled 3D gestures.\n",
        "    \"\"\"\n",
        "    speed = np.array([get_speed(p) for p in data])\n",
        "    acc = np.array([get_acc(p) for p in data])\n",
        "    angle = np.array([get_angle(p) for p in data])\n",
        "    return np.concatenate([speed, acc, angle], axis=1)\n",
        "\n",
        "def svm_user_independent(points_array, seq_array):\n",
        "    \"\"\"\n",
        "    Perform user-independent validation using a linear SVM classifier.\n",
        "    For each user, train on the 9 others and test on that user.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- SVM USER-INDEPENDENT ---\")\n",
        "    all_accuracies = []\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    for i in range(1, 11):  # Leave-one-user-out\n",
        "        X_train, X_test, seq_train, seq_test = cross_validation_user_independent_split(points_array, seq_array, i)\n",
        "\n",
        "        X_train_resampled = np.array([resampling(p) for p in X_train])\n",
        "        X_test_resampled = np.array([resampling(p) for p in X_test])\n",
        "\n",
        "        features_train = extract_features(X_train_resampled)\n",
        "        features_test = extract_features(X_test_resampled)\n",
        "\n",
        "        labels_train = np.array(seq_train, dtype=int).flatten()\n",
        "        labels_test = np.array(seq_test, dtype=int).flatten()\n",
        "\n",
        "        model = SVC(kernel=\"linear\")\n",
        "        model.fit(features_train, labels_train)\n",
        "        pred_labels_test = model.predict(features_test)\n",
        "        y_pred = np.array(pred_labels_test, dtype=int).flatten()\n",
        "\n",
        "        accuracy = accuracy_score(labels_test, y_pred)\n",
        "        all_accuracies.append(accuracy * 100)\n",
        "        all_y_true.append(labels_test)\n",
        "        all_y_pred.append(y_pred)\n",
        "\n",
        "        print(f\"User {i} accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    mean_acc = np.mean(all_accuracies)\n",
        "    std_acc = np.std(all_accuracies)\n",
        "    print(f\"\\nMoyenne SVM User-Independent: {mean_acc:.2f}%\")\n",
        "    print(f\"Écart-type SVM User-Independent: {std_acc:.2f}%\")\n",
        "\n",
        "    # Afficher les matrices de confusion après tous les scores\n",
        "    for user_id in range(10):\n",
        "        plot_conf_mat(all_y_true[user_id], all_y_pred[user_id], user_id)\n",
        "\n",
        "def svm_user_dependent(points_array, seq_array):\n",
        "    \"\"\"\n",
        "    Perform user-dependent validation using a linear SVM classifier.\n",
        "    For each user, test 10 times by holding out one repetition per digit.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- SVM USER-DEPENDENT ---\")\n",
        "    all_user_means = []\n",
        "    all_user_stds = []\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    for j in range(1, 11):  # for each user\n",
        "        user, lab = get_user(points_array, seq_array, j)\n",
        "        accs_user = []\n",
        "        y_true_user = []\n",
        "        y_pred_user = []\n",
        "\n",
        "        for i in range(1, 11):\n",
        "            X_train, X_test, labt, labest = cross_validation_user_dependent_split(user, lab, i)\n",
        "\n",
        "            X_train_resampled = np.array([resampling(p) for p in X_train])\n",
        "            X_test_resampled = np.array([resampling(p) for p in X_test])\n",
        "\n",
        "            features_train = extract_features(X_train_resampled)\n",
        "            features_test = extract_features(X_test_resampled)\n",
        "\n",
        "            model = SVC(kernel=\"linear\")\n",
        "            model.fit(features_train, labt.tolist())\n",
        "            pred_labels_test = model.predict(features_test)\n",
        "\n",
        "            y_true = np.array(labest, dtype=int).flatten()\n",
        "            y_pred = np.array(pred_labels_test, dtype=int).flatten()\n",
        "            acc = accuracy_score(y_true, y_pred)\n",
        "            accs_user.append(acc)\n",
        "\n",
        "            y_true_user.append(y_true)\n",
        "            y_pred_user.append(y_pred)\n",
        "\n",
        "            print(f\"User {j} – try {i}: {acc*100:.2f}%\")\n",
        "\n",
        "        # Moyenne et écart-type des 10 essais du user j\n",
        "        mean_user = np.mean(accs_user)\n",
        "        std_user = np.std(accs_user)\n",
        "        all_user_means.append(mean_user)\n",
        "        all_user_stds.append(std_user)\n",
        "\n",
        "        print(f\"→ Moyenne User {j}: {mean_user*100:.2f}%, Std: {std_user*100:.2f}%\")\n",
        "\n",
        "        # Concaténer tous les y_true / y_pred du user j (pour matrice)\n",
        "        all_y_true.append(np.concatenate(y_true_user))\n",
        "        all_y_pred.append(np.concatenate(y_pred_user))\n",
        "\n",
        "    # Affichage des matrices de confusion après tous les utilisateurs\n",
        "    for user_id in range(10):\n",
        "        plot_conf_mat(all_y_true[user_id], all_y_pred[user_id], user_id)\n"
      ],
      "metadata": {
        "id": "OBxj0Yv4fU40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "Simulation of SVM\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "5U3FoHTYSK0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Charger les données du Domaine 1\n",
        "    data = read_files(domain=1)\n",
        "    points_array = data[1]  # [x, y, z, t]\n",
        "    seq_array = data[2]     # labels\n",
        "\n",
        "    # Exécution de la validation user-dependent\n",
        "    svm_user_dependent(points_array, seq_array)\n",
        "\n",
        "    # Exécution de la validation user-independent\n",
        "    svm_user_independent(points_array, seq_array)\n"
      ],
      "metadata": {
        "id": "INnt9ddopoae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}